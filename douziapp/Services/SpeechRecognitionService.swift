//
//  SpeechRecognitionService.swift
//  douziapp
//
//  é«˜ç²¾åº¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹
//

import Foundation
import Speech
import AVFoundation

@MainActor
class SpeechRecognitionService: ObservableObject {
    // MARK: - Published Properties

    @Published var recognizedText: String = ""
    @Published var isListening: Bool = false
    @Published var errorMessage: String = ""
    @Published var authorizationStatus: String = "æœªç¢ºèª"
    @Published var confidenceLevel: Float = 0.0

    // MARK: - Private Properties

    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine: AVAudioEngine?
    private var currentLocale: String = "en-US"

    // MARK: - Initialization

    init() {
        setupRecognizer(locale: "en-US")
    }

    private func setupRecognizer(locale: String) {
        currentLocale = locale
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: locale))
        speechRecognizer?.defaultTaskHint = .dictation
        audioEngine = AVAudioEngine()
    }

    /// èªè­˜è¨€èªã‚’å¤‰æ›´
    func setLanguage(_ localeIdentifier: String) {
        setupRecognizer(locale: localeIdentifier)
        print("ğŸŒ éŸ³å£°èªè­˜è¨€èªã‚’å¤‰æ›´: \(localeIdentifier)")
    }

    // MARK: - Authorization

    func requestAuthorization() async -> Bool {
        // ãƒã‚¤ã‚¯æ¨©é™
        let micGranted = await requestMicrophonePermission()
        if !micGranted {
            errorMessage = "ãƒã‚¤ã‚¯ã®æ¨©é™ãŒå¿…è¦ã§ã™"
            authorizationStatus = "ãƒã‚¤ã‚¯æ‹’å¦"
            return false
        }

        // éŸ³å£°èªè­˜æ¨©é™
        let speechGranted = await requestSpeechPermission()
        if !speechGranted {
            errorMessage = "éŸ³å£°èªè­˜ã®æ¨©é™ãŒå¿…è¦ã§ã™"
            authorizationStatus = "éŸ³å£°èªè­˜æ‹’å¦"
            return false
        }

        authorizationStatus = "è¨±å¯æ¸ˆã¿"
        return true
    }

    private func requestMicrophonePermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVAudioApplication.requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
    }

    private func requestSpeechPermission() async -> Bool {
        await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status == .authorized)
            }
        }
    }

    // MARK: - é«˜ç²¾åº¦éŸ³å£°èªè­˜

    func startListening() throws {
        stopListening()
        errorMessage = ""
        recognizedText = ""
        confidenceLevel = 0.0

        audioEngine = AVAudioEngine()
        guard let audioEngine = audioEngine else {
            throw SpeechError.audioEngineError
        }

        // é«˜å“è³ªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // é«˜ç²¾åº¦èªè­˜ã®ãŸã‚ã®è¨­å®š
            try audioSession.setCategory(.playAndRecord,
                                         mode: .measurement,
                                         options: [.defaultToSpeaker, .allowBluetooth, .mixWithOthers])
            try audioSession.setPreferredSampleRate(44100.0)  // é«˜ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
            try audioSession.setPreferredIOBufferDuration(0.005)  // ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            errorMessage = "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"
            throw SpeechError.audioSessionError
        }

        // é«˜ç²¾åº¦èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw SpeechError.requestCreationFailed
        }

        // ç²¾åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹è¨­å®š
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.addsPunctuation = true  // å¥èª­ç‚¹ã‚’è¿½åŠ 

        // ã‚ªãƒ³ãƒ‡ãƒã‚¤ã‚¹èªè­˜ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ä½¿ç”¨ï¼ˆã‚ˆã‚Šé«˜é€Ÿãƒ»é«˜ç²¾åº¦ï¼‰
        if #available(iOS 13, *) {
            if speechRecognizer?.supportsOnDeviceRecognition == true {
                recognitionRequest.requiresOnDeviceRecognition = false // ã‚¯ãƒ©ã‚¦ãƒ‰ã®æ–¹ãŒç²¾åº¦ãŒé«˜ã„
            }
        }

        // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ï¼ˆèªè­˜ç²¾åº¦å‘ä¸Šï¼‰
        if currentLocale.hasPrefix("en") {
            recognitionRequest.contextualStrings = [
                "hello", "thank you", "please", "excuse me",
                "how are you", "nice to meet you", "goodbye",
                "where is", "what time", "how much"
            ]
        } else if currentLocale.hasPrefix("ja") {
            recognitionRequest.contextualStrings = [
                "ã“ã‚“ã«ã¡ã¯", "ã‚ã‚ŠãŒã¨ã†", "ãŠé¡˜ã„ã—ã¾ã™", "ã™ã¿ã¾ã›ã‚“",
                "ãŠå…ƒæ°—ã§ã™ã‹", "ã¯ã˜ã‚ã¾ã—ã¦", "ã•ã‚ˆã†ãªã‚‰",
                "ã©ã“ã§ã™ã‹", "ä½•æ™‚ã§ã™ã‹", "ã„ãã‚‰ã§ã™ã‹"
            ]
        }

        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            errorMessage = "éŸ³å£°èªè­˜ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            throw SpeechError.recognizerUnavailable
        }

        // èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            Task { @MainActor in
                guard let self = self else { return }

                if let result = result {
                    // æœ€ã‚‚ä¿¡é ¼åº¦ã®é«˜ã„çµæœã‚’ä½¿ç”¨
                    let bestTranscription = result.bestTranscription
                    self.recognizedText = bestTranscription.formattedString

                    // ä¿¡é ¼åº¦ã‚’è¨ˆç®—
                    if let segment = bestTranscription.segments.last {
                        self.confidenceLevel = segment.confidence
                    }

                    print("ğŸ¤ èªè­˜çµæœ: \(self.recognizedText) (ä¿¡é ¼åº¦: \(self.confidenceLevel))")
                }

                if let error = error {
                    self.handleRecognitionError(error)
                }
            }
        }

        // é«˜å“è³ªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå…¥åŠ›è¨­å®š
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)

        guard recordingFormat.sampleRate > 0 else {
            errorMessage = "ç„¡åŠ¹ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"
            throw SpeechError.audioEngineError
        }

        // å¤§ãã‚ã®ãƒãƒƒãƒ•ã‚¡ã§å®‰å®šã—ãŸèªè­˜
        inputNode.installTap(onBus: 0, bufferSize: 2048, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }

        do {
            audioEngine.prepare()
            try audioEngine.start()
            isListening = true
            print("ğŸ™ï¸ é«˜ç²¾åº¦éŸ³å£°èªè­˜é–‹å§‹ (\(currentLocale))")
        } catch {
            errorMessage = "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•ã‚¨ãƒ©ãƒ¼"
            throw SpeechError.audioEngineError
        }
    }

    private func handleRecognitionError(_ error: Error) {
        let nsError = error as NSError

        // æ­£å¸¸çµ‚äº†ã‚³ãƒ¼ãƒ‰ã¯ç„¡è¦–ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„ï¼‰
        let normalCodes = [203, 209, 216, 301, 1110, 1700]
        if nsError.domain == "kAFAssistantErrorDomain" && normalCodes.contains(nsError.code) {
            print("ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº† (ã‚³ãƒ¼ãƒ‰: \(nsError.code))")
            // æ­£å¸¸çµ‚äº†æ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒªã‚¢
            errorMessage = ""
            return
        }

        errorMessage = "èªè­˜ã‚¨ãƒ©ãƒ¼"
        print("âŒ èªè­˜ã‚¨ãƒ©ãƒ¼: \(error)")

        // 3ç§’å¾Œã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è‡ªå‹•ã‚¯ãƒªã‚¢
        Task {
            try? await Task.sleep(nanoseconds: 3_000_000_000)
            await MainActor.run {
                if self.errorMessage == "èªè­˜ã‚¨ãƒ©ãƒ¼" {
                    self.errorMessage = ""
                }
            }
        }
    }

    func stopListening() {
        audioEngine?.stop()
        audioEngine?.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        recognitionTask?.cancel()
        recognitionTask = nil
        isListening = false
        errorMessage = "" // åœæ­¢æ™‚ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒªã‚¢
        print("â¹ï¸ éŸ³å£°èªè­˜åœæ­¢")
    }

    func clearText() {
        recognizedText = ""
        errorMessage = ""
        confidenceLevel = 0.0
    }
}

// MARK: - Errors

enum SpeechError: LocalizedError {
    case notAuthorized, recognizerUnavailable, requestCreationFailed
    case audioSessionError, audioEngineError

    var errorDescription: String? {
        switch self {
        case .notAuthorized: return "éŸ³å£°èªè­˜ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        case .recognizerUnavailable: return "éŸ³å£°èªè­˜ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        case .requestCreationFailed: return "èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—"
        case .audioSessionError: return "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"
        case .audioEngineError: return "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ³ã‚¸ãƒ³ã‚¨ãƒ©ãƒ¼"
        }
    }
}
